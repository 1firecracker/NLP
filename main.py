"""
GSM8K Baseline ä¸»æ‰§è¡Œè„šæœ¬
"""
import os
import sys
import json
import argparse
import time
import asyncio
from typing import List, Dict, Any
from pathlib import Path

# å¯¼å…¥é¡¹ç›®æ¨¡å—
from data.config import config
from core.llm_client import LLMClient, token_tracker
from core.baseline import nshot_chats
from core.evaluation import extract_ans_from_response
from processing.concurrent_processor import ConcurrentProcessor
from analysis.method_comparator import MethodComparator, MethodResult

class BaselineRunner:
    """Baselineæ‰§è¡Œå™¨"""
    
    def __init__(self):
        """åˆå§‹åŒ–æ‰§è¡Œå™¨"""
        self.client = LLMClient()
        self.results = []
        
    def load_test_data(self, file_path: str) -> List[Dict[str, Any]]:
        """
        åŠ è½½æµ‹è¯•æ•°æ®
        
        Args:
            file_path: æµ‹è¯•æ–‡ä»¶è·¯å¾„
            
        Returns:
            List[Dict]: æµ‹è¯•æ•°æ®åˆ—è¡¨
        """
        data = []
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                for line in f:
                    if line.strip():
                        data.append(json.loads(line.strip()))
            print(f"âœ… æˆåŠŸåŠ è½½ {len(data)} æ¡æµ‹è¯•æ•°æ®")
            return data
        except FileNotFoundError:
            print(f"âŒ æµ‹è¯•æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
            return []
        except Exception as e:
            print(f"âŒ åŠ è½½æµ‹è¯•æ•°æ®å¤±è´¥: {e}")
            return []
    
    async def run_zero_shot(self, test_data: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """è¿è¡ŒZero-shot baselineï¼ˆä½¿ç”¨å¹¶å‘å¤„ç†ï¼‰"""
        print(f"\nğŸš€ å¼€å§‹Zero-shot baseline...")
        
        # åˆ›å»ºå¹¶å‘å¤„ç†å™¨
        processor = ConcurrentProcessor(max_concurrent=5, rate_limit=100)
        
        # å¹¶å‘å¤„ç†
        results = await processor.process_batch(test_data, method="zero-shot")
        
        # è½¬æ¢ä¸ºæ ‡å‡†æ ¼å¼
        standard_results = []
        for result in results:
            standard_results.append({
                'question': result.question,
                'ground_truth': result.ground_truth,
                'predicted_answer': result.predicted_answer,
                'response': result.response,
                'token_stats': result.token_stats,
                'correct': result.correct,
                'processing_time': result.processing_time,
                'error': result.error
            })
        
        print(f"âœ… Zero-shot baselineå®Œæˆï¼Œå¤„ç†äº† {len(standard_results)} ä¸ªé—®é¢˜")
        return standard_results
    
    async def run_few_shot(self, test_data: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """è¿è¡ŒFew-shot baselineï¼ˆä½¿ç”¨å¹¶å‘å¤„ç†ï¼‰"""
        print(f"\nğŸš€ å¼€å§‹Few-shot baseline...")
        
        # åˆ›å»ºå¹¶å‘å¤„ç†å™¨
        processor = ConcurrentProcessor(max_concurrent=5, rate_limit=100)
        
        # å¹¶å‘å¤„ç†
        results = await processor.process_batch(test_data, method="few-shot")
        
        # è½¬æ¢ä¸ºæ ‡å‡†æ ¼å¼
        standard_results = []
        for result in results:
            standard_results.append({
                'question': result.question,
                'ground_truth': result.ground_truth,
                'predicted_answer': result.predicted_answer,
                'response': result.response,
                'token_stats': result.token_stats,
                'correct': result.correct,
                'processing_time': result.processing_time,
                'error': result.error
            })
        
        print(f"âœ… Few-shot baselineå®Œæˆï¼Œå¤„ç†äº† {len(standard_results)} ä¸ªé—®é¢˜")
        return standard_results
    
    async def run_concurrent_baseline(self, test_data: List[Dict[str, Any]], method: str = "zero-shot") -> List[Dict[str, Any]]:
        """
        è¿è¡Œå¹¶å‘baseline
        
        Args:
            test_data: æµ‹è¯•æ•°æ®
            method: å¤„ç†æ–¹æ³•
            
        Returns:
            List[Dict]: ç»“æœåˆ—è¡¨
        """
        print(f"\nğŸš€ å¼€å§‹å¹¶å‘{method} baseline...")
        
        # åˆ›å»ºå¹¶å‘å¤„ç†å™¨ï¼ˆä¼˜åŒ–é€Ÿç‡é™åˆ¶ï¼‰
        processor = ConcurrentProcessor(max_concurrent=5, rate_limit=100)
        
        # å¹¶å‘å¤„ç†
        results = await processor.process_batch(test_data, method=method)
        
        # è½¬æ¢ä¸ºæ ‡å‡†æ ¼å¼
        standard_results = []
        for result in results:
            standard_results.append({
                'question': result.question,
                'ground_truth': result.ground_truth,
                'predicted_answer': result.predicted_answer,
                'response': result.response,
                'token_stats': result.token_stats,
                'correct': result.correct,
                'processing_time': result.processing_time,
                'error': result.error
            })
        
        print(f"âœ… å¹¶å‘{method} baselineå®Œæˆï¼Œå¤„ç†äº† {len(standard_results)} ä¸ªé—®é¢˜")
        return standard_results
    
    async def run_progressive_hint(self, test_data: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """è¿è¡ŒProgressive-Hint baselineï¼ˆä½¿ç”¨å¹¶å‘å¤„ç†ï¼‰"""
        print(f"\nğŸš€ å¼€å§‹Progressive-Hint baseline...")
        
        # åˆ›å»ºå¹¶å‘å¤„ç†å™¨
        processor = ConcurrentProcessor(max_concurrent=5, rate_limit=100)
        
        # å¹¶å‘å¤„ç†
        results = await processor.process_batch(test_data, method="progressive-hint")
        
        # è½¬æ¢ä¸ºæ ‡å‡†æ ¼å¼
        standard_results = []
        for result in results:
            standard_results.append({
                'question': result.question,
                'ground_truth': result.ground_truth,
                'predicted_answer': result.predicted_answer,
                'response': result.response,
                'token_stats': result.token_stats,
                'correct': result.correct,
                'processing_time': result.processing_time,
                'error': result.error
            })
        
        print(f"âœ… Progressive-Hint baselineå®Œæˆï¼Œå¤„ç†äº† {len(standard_results)} ä¸ªé—®é¢˜")
        return standard_results
    
    async def run_program_of_thoughts(self, test_data: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """è¿è¡ŒProgram of Thoughts baselineï¼ˆä½¿ç”¨å¹¶å‘å¤„ç†ï¼‰"""
        print(f"\nğŸš€ å¼€å§‹Program of Thoughts baseline...")
        
        # åˆ›å»ºå¹¶å‘å¤„ç†å™¨ï¼ˆä¼˜åŒ–é€Ÿç‡é™åˆ¶ï¼‰
        processor = ConcurrentProcessor(max_concurrent=5, rate_limit=100)
        
        # å¹¶å‘å¤„ç†
        results = await processor.process_batch(test_data, method="program-of-thoughts")
        
        # è½¬æ¢ä¸ºæ ‡å‡†æ ¼å¼
        standard_results = []
        for result in results:
            standard_results.append({
                'question': result.question,
                'ground_truth': result.ground_truth,
                'predicted_answer': result.predicted_answer,
                'response': result.response,
                'token_stats': result.token_stats,
                'correct': result.correct,
                'processing_time': result.processing_time,
                'error': result.error
            })
        
        print(f"âœ… Program of Thoughts baselineå®Œæˆï¼Œå¤„ç†äº† {len(standard_results)} ä¸ªé—®é¢˜")
        return standard_results
    
    async def run_hybrid_pot_php(self, test_data: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """è¿è¡Œæ··åˆPoTå’ŒPHP baselineï¼ˆä½¿ç”¨å¹¶å‘å¤„ç†ï¼‰"""
        print(f"\nğŸš€ å¼€å§‹æ··åˆPoTå’ŒPHP baseline...")
        
        # åˆ›å»ºå¹¶å‘å¤„ç†å™¨
        processor = ConcurrentProcessor(max_concurrent=5, rate_limit=100)
        
        # å¹¶å‘å¤„ç†
        results = await processor.process_batch(test_data, method="hybrid-pot-php")
        
        # è½¬æ¢ä¸ºæ ‡å‡†æ ¼å¼
        standard_results = []
        for result in results:
            standard_results.append({
                'question': result.question,
                'ground_truth': result.ground_truth,
                'predicted_answer': result.predicted_answer,
                'response': result.response,
                'token_stats': result.token_stats,
                'correct': result.correct,
                'processing_time': result.processing_time,
                'error': result.error
            })
        
        print(f"âœ… æ··åˆPoTå’ŒPHP baselineå®Œæˆï¼Œå¤„ç†äº† {len(standard_results)} ä¸ªé—®é¢˜")
        return standard_results
    
    def save_results(self, results: List[Dict[str, Any]], filename: str):
        """
        ä¿å­˜ç»“æœåˆ°æ–‡ä»¶
        
        Args:
            results: ç»“æœåˆ—è¡¨
            filename: è¾“å‡ºæ–‡ä»¶å
        """
        # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
        os.makedirs(config.OUTPUT_DIR, exist_ok=True)
        
        file_path = os.path.join(config.OUTPUT_DIR, filename)
        
        try:
            with open(file_path, 'w', encoding='utf-8') as f:
                for result in results:
                    f.write(json.dumps(result, ensure_ascii=False) + '\n')
            print(f"âœ… ç»“æœå·²ä¿å­˜åˆ°: {file_path}")
        except Exception as e:
            print(f"âŒ ä¿å­˜ç»“æœå¤±è´¥: {e}")
    
    def calculate_accuracy(self, results: List[Dict[str, Any]]) -> float:
        """
        è®¡ç®—å‡†ç¡®ç‡
        
        Args:
            results: ç»“æœåˆ—è¡¨
            
        Returns:
            float: å‡†ç¡®ç‡
        """
        correct_count = sum(1 for result in results if result.get('correct', False))
        total_count = len(results)
        accuracy = correct_count / total_count if total_count > 0 else 0.0
        return accuracy
    
    def generate_analysis_report(self, results: List[Dict[str, Any]], method_name: str, 
                                processing_time: float = 0.0) -> Dict[str, Any]:
        """
        ç”Ÿæˆå•æ¬¡ç»“æœåˆ†ææŠ¥å‘Š
        
        Args:
            results: ç»“æœåˆ—è¡¨
            method_name: æ–¹æ³•åç§°
            processing_time: å¤„ç†æ—¶é—´
            
        Returns:
            Dict: åˆ†ææŠ¥å‘Š
        """
        if not results:
            return {}
        
        # åŸºç¡€ç»Ÿè®¡
        total_questions = len(results)
        correct_answers = sum(1 for r in results if r.get('correct', False))
        accuracy = correct_answers / total_questions if total_questions > 0 else 0.0
        
        # Tokenç»Ÿè®¡
        total_input_tokens = sum(r.get('token_stats', {}).get('prompt_tokens', 0) for r in results)
        total_output_tokens = sum(r.get('token_stats', {}).get('completion_tokens', 0) for r in results)
        total_tokens = total_input_tokens + total_output_tokens
        avg_tokens_per_question = total_tokens / total_questions if total_questions > 0 else 0.0
        
        # æ—¶é—´ç»Ÿè®¡
        avg_processing_time = processing_time / total_questions if total_questions > 0 else 0.0
        
        # æˆæœ¬åˆ†æ (å‡è®¾ä»·æ ¼ï¼Œå¯æ ¹æ®å®é™…APIè°ƒæ•´)
        input_token_cost_per_k = 0.0005  # $0.5 per 1K tokens
        output_token_cost_per_k = 0.0015  # $1.5 per 1K tokens
        estimated_cost = (total_input_tokens / 1000 * input_token_cost_per_k) + \
                        (total_output_tokens / 1000 * output_token_cost_per_k)
        
        # é”™è¯¯åˆ†æ
        error_count = sum(1 for r in results if r.get('error'))
        error_rate = error_count / total_questions if total_questions > 0 else 0.0
        
        # ç”ŸæˆæŠ¥å‘Š
        report = {
            "method_name": method_name,
            "timestamp": time.strftime("%Y-%m-%d %H:%M:%S"),
            "hyperparameters": {
                "temperature": config.TEMPERATURE,
                "top_p": config.TOP_P,
                "max_tokens": config.MAX_TOKENS,
                "frequency_penalty": config.FREQUENCY_PENALTY,
                "presence_penalty": config.PRESENCE_PENALTY
            },
            "performance_metrics": {
                "total_questions": total_questions,
                "correct_answers": correct_answers,
                "accuracy": accuracy,
                "wall_clock_time": processing_time,
                "avg_processing_time_per_question": avg_processing_time
            },
            "token_usage": {
                "total_input_tokens": total_input_tokens,
                "total_output_tokens": total_output_tokens,
                "total_tokens": total_tokens,
                "avg_tokens_per_question": avg_tokens_per_question
            },
            "cost_analysis": {
                "estimated_cost_usd": estimated_cost,
                "cost_per_question": estimated_cost / total_questions if total_questions > 0 else 0.0
            },
            "error_analysis": {
                "error_count": error_count,
                "error_rate": error_rate
            }
        }
        
        return report
    
    def save_analysis_report(self, report: Dict[str, Any], method_name: str):
        """
        ä¿å­˜åˆ†ææŠ¥å‘Šåˆ°æ–‡ä»¶
        
        Args:
            report: åˆ†ææŠ¥å‘Š
            method_name: æ–¹æ³•åç§°
        """
        # ç”Ÿæˆæ–‡ä»¶å: æ–¹æ³•å_æ¸©åº¦_top-p_æ—¶é—´æˆ³
        timestamp = time.strftime("%Y%m%d_%H%M%S")
        temp_str = f"{config.TEMPERATURE:.1f}".replace('.', 'p')
        top_p_str = f"{config.TOP_P:.1f}".replace('.', 'p')
        
        filename = f"{method_name}_temp{temp_str}_topp{top_p_str}_{timestamp}.json"
        filepath = os.path.join(config.SUMMARY_DIR, filename)
        
        # ç¡®ä¿ç›®å½•å­˜åœ¨
        os.makedirs(config.SUMMARY_DIR, exist_ok=True)
        
        # ä¿å­˜æŠ¥å‘Š
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(report, f, indent=2, ensure_ascii=False)
        
        print(f"ğŸ“Š åˆ†ææŠ¥å‘Šå·²ä¿å­˜åˆ°: {filepath}")
        
        # æ‰“å°ç®€è¦ç»Ÿè®¡
        metrics = report['performance_metrics']
        token_usage = report['token_usage']
        cost_analysis = report['cost_analysis']
        
        print(f"ğŸ“ˆ {method_name} åˆ†æç»“æœ:")
        print(f"   - å‡†ç¡®ç‡: {metrics['accuracy']:.2%}")
        print(f"   - å¤„ç†æ—¶é—´: {metrics['wall_clock_time']:.2f}ç§’")
        print(f"   - å¹³å‡æ—¶é—´/é—®é¢˜: {metrics['avg_processing_time_per_question']:.2f}ç§’")
        print(f"   - å¹³å‡Token/é—®é¢˜: {token_usage['avg_tokens_per_question']:.1f}")
        print(f"   - é¢„ä¼°æˆæœ¬: ${cost_analysis['estimated_cost_usd']:.4f}")
        print(f"   - è¶…å‚æ•°: temp={config.TEMPERATURE}, top_p={config.TOP_P}")

async def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='GSM8K Baseline Runner')
    parser.add_argument('--test-file', default=config.TEST_FILE, help='æµ‹è¯•æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--output-dir', default=config.OUTPUT_DIR, help='è¾“å‡ºç›®å½•')
    parser.add_argument('--max-questions', type=int, default=None, help='æœ€å¤§å¤„ç†é—®é¢˜æ•°é‡')
    parser.add_argument('--method', choices=['zero-shot', 'few-shot', 'concurrent', 'progressive-hint', 'program-of-thoughts', 'hybrid-pot-php', 'all'], default='both', help='è¿è¡Œçš„æ–¹æ³•')
    parser.add_argument('--verbose', action='store_true', help='è¯¦ç»†è¾“å‡º')
    
    args = parser.parse_args()
    
    # æ›´æ–°é…ç½®
    config.TEST_FILE = args.test_file
    config.OUTPUT_DIR = args.output_dir
    config.VERBOSE = args.verbose
    
    print("ğŸ¯ GSM8K Baseline å¼€å§‹æ‰§è¡Œ")
    print("=" * 50)
    
    # éªŒè¯é…ç½®
    if not config.validate():
        print("âŒ é…ç½®éªŒè¯å¤±è´¥ï¼Œè¯·æ£€æŸ¥APIå¯†é’¥è®¾ç½®")
        return
    
    # æ‰“å°é…ç½®
    config.print_config()
    
    # æµ‹è¯•APIè¿æ¥
    runner = BaselineRunner()
    if not runner.client.test_connection():
        print("âŒ APIè¿æ¥æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥ç½‘ç»œå’ŒAPIå¯†é’¥")
        return
    
    # åŠ è½½æµ‹è¯•æ•°æ®
    test_data = runner.load_test_data(config.TEST_FILE)
    if not test_data:
        print("âŒ æ— æ³•åŠ è½½æµ‹è¯•æ•°æ®")
        return
    
    # é™åˆ¶å¤„ç†æ•°é‡ï¼ˆç”¨äºæµ‹è¯•ï¼‰
    if args.max_questions:
        test_data = test_data[:args.max_questions]
        print(f"ğŸ”¬ æµ‹è¯•æ¨¡å¼ï¼šä»…å¤„ç†å‰ {len(test_data)} ä¸ªé—®é¢˜")
    
    # è¿è¡Œbaseline
    if args.method in ['zero-shot', 'both', 'all']:
        start_time = time.time()
        zero_shot_results = await runner.run_zero_shot(test_data)
        processing_time = time.time() - start_time
        
        runner.save_results(zero_shot_results, 'zeroshot.baseline.jsonl')
        
        accuracy = runner.calculate_accuracy(zero_shot_results)
        print(f"ğŸ“Š Zero-shot å‡†ç¡®ç‡: {accuracy:.4f} ({accuracy*100:.2f}%)")
        
        # ç”Ÿæˆåˆ†ææŠ¥å‘Š
        report = runner.generate_analysis_report(zero_shot_results, "zero-shot", processing_time)
        runner.save_analysis_report(report, "zero-shot")
    
    if args.method in ['few-shot', 'both', 'all']:
        start_time = time.time()
        few_shot_results = await runner.run_few_shot(test_data)
        processing_time = time.time() - start_time
        
        runner.save_results(few_shot_results, 'fewshot.baseline.jsonl')
        
        accuracy = runner.calculate_accuracy(few_shot_results)
        print(f"ğŸ“Š Few-shot å‡†ç¡®ç‡: {accuracy:.4f} ({accuracy*100:.2f}%)")
        
        # ç”Ÿæˆåˆ†ææŠ¥å‘Š
        report = runner.generate_analysis_report(few_shot_results, "few-shot", processing_time)
        runner.save_analysis_report(report, "few-shot")
    
    if args.method in ['concurrent', 'all']:
        print("ğŸš€ å¼€å§‹å¹¶å‘å¤„ç†...")
        start_time = time.time()
        concurrent_results = asyncio.run(runner.run_concurrent_baseline(test_data, "zero-shot"))
        processing_time = time.time() - start_time
        
        runner.save_results(concurrent_results, 'concurrent.baseline.jsonl')
        
        accuracy = runner.calculate_accuracy(concurrent_results)
        print(f"ğŸ“Š å¹¶å‘Zero-shot å‡†ç¡®ç‡: {accuracy:.4f} ({accuracy*100:.2f}%)")
        
        # ç”Ÿæˆåˆ†ææŠ¥å‘Š
        report = runner.generate_analysis_report(concurrent_results, "concurrent", processing_time)
        runner.save_analysis_report(report, "concurrent")
    
    if args.method in ['progressive-hint', 'all']:
        start_time = time.time()
        progressive_hint_results = await runner.run_progressive_hint(test_data)
        processing_time = time.time() - start_time
        
        runner.save_results(progressive_hint_results, 'progressive_hint.baseline.jsonl')
        
        accuracy = runner.calculate_accuracy(progressive_hint_results)
        print(f"ğŸ“Š Progressive-Hint å‡†ç¡®ç‡: {accuracy:.4f} ({accuracy*100:.2f}%)")
        
        # ç”Ÿæˆåˆ†ææŠ¥å‘Š
        report = runner.generate_analysis_report(progressive_hint_results, "progressive-hint", processing_time)
        runner.save_analysis_report(report, "progressive-hint")
    
    if args.method in ['program-of-thoughts', 'all']:
        start_time = time.time()
        pot_results = await runner.run_program_of_thoughts(test_data)
        processing_time = time.time() - start_time
        
        runner.save_results(pot_results, 'program_of_thoughts.baseline.jsonl')
        
        accuracy = runner.calculate_accuracy(pot_results)
        print(f"ğŸ“Š Program of Thoughts å‡†ç¡®ç‡: {accuracy:.4f} ({accuracy*100:.2f}%)")
        
        # ç”Ÿæˆåˆ†ææŠ¥å‘Š
        report = runner.generate_analysis_report(pot_results, "program-of-thoughts", processing_time)
        runner.save_analysis_report(report, "program-of-thoughts")
    
    if args.method in ['hybrid-pot-php', 'all']:
        start_time = time.time()
        hybrid_results = await runner.run_hybrid_pot_php(test_data)
        processing_time = time.time() - start_time
        
        runner.save_results(hybrid_results, 'hybrid_pot_php.baseline.jsonl')
        
        accuracy = runner.calculate_accuracy(hybrid_results)
        print(f"ğŸ“Š æ··åˆPoTå’ŒPHP å‡†ç¡®ç‡: {accuracy:.4f} ({accuracy*100:.2f}%)")
        
        # ç”Ÿæˆåˆ†ææŠ¥å‘Š
        report = runner.generate_analysis_report(hybrid_results, "hybrid-pot-php", processing_time)
        runner.save_analysis_report(report, "hybrid-pot-php")
    
    # æ‰“å°tokenä½¿ç”¨ç»Ÿè®¡
    token_tracker.print_summary()
    
    print("\nğŸ‰ Baselineæ‰§è¡Œå®Œæˆï¼")

if __name__ == "__main__":
    import asyncio
    asyncio.run(main())
