"""
OpenAI API å®¢æˆ·ç«¯
"""
import time
import json
from typing import List, Dict, Any, Optional, Tuple
import openai
from openai import OpenAI
from data.config import config

class LLMClient:
    """OpenAI APIå®¢æˆ·ç«¯"""
    
    def __init__(self):
        """åˆå§‹åŒ–å®¢æˆ·ç«¯"""
        # æ”¯æŒç¡…åŸºæµåŠ¨APIç«¯ç‚¹
        if config.OPENAI_BASE_URL:
            self.client = OpenAI(
                api_key=config.OPENAI_API_KEY,
                base_url=config.OPENAI_BASE_URL
            )
        else:
            self.client = OpenAI(api_key=config.OPENAI_API_KEY)
        
        self.model = config.OPENAI_MODEL
        self.max_retries = config.MAX_RETRIES
        self.retry_delay = config.RETRY_DELAY
        
    def generate_response(self, messages: List[Dict[str, str]], **kwargs) -> Tuple[str, Dict[str, int]]:
        """
        ç”Ÿæˆå“åº”
        
        Args:
            messages: å¯¹è¯æ¶ˆæ¯åˆ—è¡¨
            **kwargs: é¢å¤–çš„ç”Ÿæˆå‚æ•°
            
        Returns:
            Tuple[str, Dict[str, int]]: (å“åº”æ–‡æœ¬, tokenç»Ÿè®¡ä¿¡æ¯)
        """
        # è®¾ç½®ç”Ÿæˆå‚æ•°
        generation_params = {
            "model": self.model,
            "messages": messages,
            "temperature": kwargs.get("temperature", config.TEMPERATURE),
            "top_p": kwargs.get("top_p", config.TOP_P),
            "max_tokens": kwargs.get("max_tokens", config.MAX_TOKENS),
            "frequency_penalty": kwargs.get("frequency_penalty", config.FREQUENCY_PENALTY),
            "presence_penalty": kwargs.get("presence_penalty", config.PRESENCE_PENALTY),
        }
        
        # é‡è¯•æœºåˆ¶
        for attempt in range(self.max_retries):
            try:
                response = self.client.chat.completions.create(**generation_params)
                
                # æå–å“åº”å†…å®¹
                response_text = response.choices[0].message.content
                
                # ç»Ÿè®¡tokenä½¿ç”¨é‡
                token_stats = {
                    "prompt_tokens": response.usage.prompt_tokens,
                    "completion_tokens": response.usage.completion_tokens,
                    "total_tokens": response.usage.total_tokens
                }
                
                if config.VERBOSE:
                    print(f"âœ… APIè°ƒç”¨æˆåŠŸ (å°è¯• {attempt + 1}/{self.max_retries})")
                    print(f"ğŸ“Š Tokenä½¿ç”¨: {token_stats['total_tokens']} (è¾“å…¥: {token_stats['prompt_tokens']}, è¾“å‡º: {token_stats['completion_tokens']})")
                
                return response_text, token_stats
                
            except openai.RateLimitError as e:
                print(f"âš ï¸  APIé™æµé”™è¯¯ (å°è¯• {attempt + 1}/{self.max_retries}): {e}")
                if attempt < self.max_retries - 1:
                    wait_time = self.retry_delay * (2 ** attempt)  # æŒ‡æ•°é€€é¿
                    print(f"â³ ç­‰å¾… {wait_time} ç§’åé‡è¯•...")
                    time.sleep(wait_time)
                else:
                    raise e
                    
            except openai.APIError as e:
                print(f"âŒ APIé”™è¯¯ (å°è¯• {attempt + 1}/{self.max_retries}): {e}")
                if attempt < self.max_retries - 1:
                    time.sleep(self.retry_delay)
                else:
                    raise e
                    
            except Exception as e:
                print(f"âŒ æœªçŸ¥é”™è¯¯ (å°è¯• {attempt + 1}/{self.max_retries}): {e}")
                if attempt < self.max_retries - 1:
                    time.sleep(self.retry_delay)
                else:
                    raise e
    
    def count_tokens(self, text: str) -> int:
        """
        è®¡ç®—æ–‡æœ¬çš„tokenæ•°é‡
        
        Args:
            text: è¾“å…¥æ–‡æœ¬
            
        Returns:
            int: tokenæ•°é‡
        """
        try:
            # ä½¿ç”¨tiktokenè¿›è¡Œtokenè®¡æ•°
            import tiktoken
            encoding = tiktoken.encoding_for_model(self.model)
            return len(encoding.encode(text))
        except ImportError:
            # å¦‚æœæ²¡æœ‰tiktokenï¼Œä½¿ç”¨ç®€å•ä¼°ç®—
            return len(text.split()) * 1.3  # ç²—ç•¥ä¼°ç®—
    
    def test_connection(self) -> bool:
        """
        æµ‹è¯•APIè¿æ¥
        
        Returns:
            bool: è¿æ¥æ˜¯å¦æˆåŠŸ
        """
        try:
            test_messages = [
                {"role": "user", "content": "Hello, this is a test message."}
            ]
            response, _ = self.generate_response(test_messages, max_tokens=10)
            print("âœ… APIè¿æ¥æµ‹è¯•æˆåŠŸ")
            return True
        except Exception as e:
            print(f"âŒ APIè¿æ¥æµ‹è¯•å¤±è´¥: {e}")
            return False

class TokenTracker:
    """Tokenä½¿ç”¨é‡è·Ÿè¸ªå™¨"""
    
    def __init__(self):
        self.total_prompt_tokens = 0
        self.total_completion_tokens = 0
        self.total_tokens = 0
        self.api_calls = 0
        
    def add_usage(self, token_stats: Dict[str, int]):
        """æ·»åŠ tokenä½¿ç”¨ç»Ÿè®¡"""
        self.total_prompt_tokens += token_stats.get("prompt_tokens", 0)
        self.total_completion_tokens += token_stats.get("completion_tokens", 0)
        self.total_tokens += token_stats.get("total_tokens", 0)
        self.api_calls += 1
        
    def get_summary(self) -> Dict[str, Any]:
        """è·å–ä½¿ç”¨ç»Ÿè®¡æ‘˜è¦"""
        return {
            "total_prompt_tokens": self.total_prompt_tokens,
            "total_completion_tokens": self.total_completion_tokens,
            "total_tokens": self.total_tokens,
            "api_calls": self.api_calls,
            "average_tokens_per_call": self.total_tokens / max(self.api_calls, 1)
        }
    
    def print_summary(self):
        """æ‰“å°ä½¿ç”¨ç»Ÿè®¡"""
        summary = self.get_summary()
        print("\nğŸ“Š Tokenä½¿ç”¨ç»Ÿè®¡:")
        print(f"  - æ€»APIè°ƒç”¨æ¬¡æ•°: {summary['api_calls']}")
        print(f"  - æ€»è¾“å…¥Token: {summary['total_prompt_tokens']}")
        print(f"  - æ€»è¾“å‡ºToken: {summary['total_completion_tokens']}")
        print(f"  - æ€»Tokenä½¿ç”¨: {summary['total_tokens']}")
        print(f"  - å¹³å‡æ¯æ¬¡è°ƒç”¨Token: {summary['average_tokens_per_call']:.1f}")

# å…¨å±€tokenè·Ÿè¸ªå™¨
token_tracker = TokenTracker()
